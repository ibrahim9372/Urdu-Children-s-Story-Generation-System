{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Notebook: 01 - Cleaning\n",
    "\n",
    "Purpose: conservative, reversible cleaning pipeline. Run the pipeline cell to regenerate `urdu_stories_final_preprocessed.json`.\n",
    "\n",
    "**Character filtering**: This notebook removes only sentences (text between `<EOS>` markers) that contain non-Ukdu characters (Latin letters, digits, etc.). Pure Urdu sentences are preserved intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & paths\n",
    "import json, re, unicodedata\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PHASE1_ROOT = NOTEBOOK_DIR\n",
    "SRC_JSON = PHASE1_ROOT / 'urdu_stories_final.json'\n",
    "if not SRC_JSON.exists():\n",
    "    raise FileNotFoundError(f\"{SRC_JSON} not found — run data-collection to create latest dataset\")\n",
    "CLEAN_DIR = PHASE1_ROOT\n",
    "CLEAN_JSON = CLEAN_DIR / 'urdu_stories_final_preprocessed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cleaning_utils",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning utilities defined\n"
     ]
    }
   ],
   "source": [
    "# Core normalization and cleaning utilities\n",
    "import re, unicodedata\n",
    "\n",
    "# Define Urdu Unicode range\n",
    "URDU_RANGE = range(0x0600, 0x06FF + 1)\n",
    "\n",
    "# Characters to KEEP (besides Urdu)\n",
    "KEEP_CHARS = {'۔', '؟', '،', '؛', '!', ' '}\n",
    "\n",
    "def is_urdu_or_allowed(char):\n",
    "    \"\"\"Check if character is Urdu or explicitly allowed.\"\"\"\n",
    "    if char in KEEP_CHARS:\n",
    "        return True\n",
    "    if ord(char) in URDU_RANGE:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def sentence_has_non_urdu(sentence):\n",
    "    \"\"\"Check if a sentence contains non-Ukdu characters (excluding special tokens).\"\"\"\n",
    "    # Remove special tokens for checking\n",
    "    temp = sentence\n",
    "    for tok in ['<EOS>', '<EOP>', '<EOT>']:\n",
    "        temp = temp.replace(tok, '')\n",
    "    \n",
    "    # Check each character\n",
    "    for char in temp:\n",
    "        if not is_urdu_or_allowed(char):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_sentences_with_non_urdu(text):\n",
    "    \"\"\"Remove sentences that contain non-Ukdu characters.\"\"\"\n",
    "    # Split by EOS marker\n",
    "    parts = text.split('<EOS>')\n",
    "    \n",
    "    clean_parts = []\n",
    "    removed_count = 0\n",
    "    \n",
    "    for part in parts:\n",
    "        # Check if this part (sentence) has non-Ukdu\n",
    "        if sentence_has_non_urdu(part):\n",
    "            removed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Keep the part and add back EOS marker\n",
    "        if part.strip():\n",
    "            clean_parts.append(part.strip())\n",
    "    \n",
    "    # Rejoin with EOS markers\n",
    "    result = ' <EOS> '.join(clean_parts)\n",
    "    \n",
    "    # Ensure EOT at the end\n",
    "    if '<EOT>' not in result:\n",
    "        result = result.strip() + ' <EOT>'\n",
    "    \n",
    "    return result.strip(), removed_count\n",
    "\n",
    "\n",
    "_DIACRITICS_RE = re.compile('[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]')\n",
    "\n",
    "def normalize_urdu(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    t = unicodedata.normalize('NFC', text)\n",
    "    t = _DIACRITICS_RE.sub('', t)\n",
    "    t = re.sub('[\\u0622\\u0623\\u0625\\u0671]', '\\u0627', t)  # Alef variants\n",
    "    t = re.sub('\\u0643', '\\u06A9', t)  # ك -> ک\n",
    "    t = re.sub('\\u064A', '\\u06CC', t)  # ي -> ی\n",
    "    t = re.sub('[\\u200C\\u200D\\uFEFF]', '', t)\n",
    "    t = re.sub('\\u0001', '\\u06D4', t)  # Fix SOH -> Urdu full stop\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def collapse_duplicate_markers(text: str) -> (str, int):\n",
    "    t = text\n",
    "    changed = 0\n",
    "    for tok in ['<EOS>','<EOP>','<EOT>']:\n",
    "        pattern = re.compile(r'(?:' + re.escape(tok) + r')[\\s\\n]*(?:' + re.escape(tok) + r')+')\n",
    "        t, n = pattern.subn(tok, t)\n",
    "        changed += n\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    return t.strip(), changed\n",
    "\n",
    "\n",
    "_TERMINATOR_RE = re.compile(r'([\\u06D4\\u061F\\.\\!\\?])(?!\\s*<EOS>)')\n",
    "\n",
    "def insert_missing_eos(text: str) -> (str, int):\n",
    "    inserts = 0\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for m in _TERMINATOR_RE.finditer(text):\n",
    "        pos = m.start(1)\n",
    "        prev_slice = text[last:pos+1]\n",
    "        if len(prev_slice.strip()) >= 8:\n",
    "            after = text[m.end(1): m.end(1)+10]\n",
    "            if '<EOS>' not in after:\n",
    "                inserts += 1\n",
    "                parts.append(text[last:m.end(1)] + ' <EOS>')\n",
    "                last = m.end(1)\n",
    "    parts.append(text[last:])\n",
    "    if inserts:\n",
    "        return ''.join(parts), inserts\n",
    "    return text, 0\n",
    "\n",
    "\n",
    "def clean_story_text(orig: str) -> (str, dict):\n",
    "    t = orig\n",
    "    changes = {'normalized': False, 'collapsed_markers': 0, 'inserted_eos': 0, 'added_eot': 0, 'removed_sentences': 0}\n",
    "    \n",
    "    # Step 1: Normalize Urdu (before sentence removal)\n",
    "    norm = normalize_urdu(t)\n",
    "    if norm != t:\n",
    "        changes['normalized'] = True\n",
    "        t = norm\n",
    "    \n",
    "    # Step 2: Collapse duplicate markers\n",
    "    t, collapsed = collapse_duplicate_markers(t)\n",
    "    changes['collapsed_markers'] = collapsed\n",
    "    \n",
    "    # Step 3: Remove sentences with non-Ukdu characters\n",
    "    t, removed = remove_sentences_with_non_urdu(t)\n",
    "    changes['removed_sentences'] = removed\n",
    "    \n",
    "    # Step 4: Insert missing EOS\n",
    "    t, inserted = insert_missing_eos(t)\n",
    "    changes['inserted_eos'] = inserted\n",
    "    \n",
    "    # Step 5: Add EOT if missing\n",
    "    if '<EOT>' not in t:\n",
    "        t = t.strip() + ' <EOT>'\n",
    "        changes['added_eot'] = 1\n",
    "    \n",
    "    # Step 6: Clean up whitespace\n",
    "    t = re.sub(r'\\s*<\\s*(EOP|EOS|EOT)\\s*>\\s*', r' <\\1> ', t)\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t, changes\n",
    "\n",
    "\n",
    "print('Cleaning utilities defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "run_clean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean complete — stories: 1780\n",
      "Sentences removed: 19561\n"
     ]
    }
   ],
   "source": [
    "# Run cleaning pipeline\n",
    "with open(SRC_JSON,'r',encoding='utf-8') as f:\n",
    "    orig_stories = json.load(f)\n",
    "\n",
    "cleaned = []\n",
    "change_log = []\n",
    "summary_counts = Counter()\n",
    "for i,s in enumerate(orig_stories):\n",
    "    content = s.get('content','')\n",
    "    new_content, changes = clean_story_text(content)\n",
    "    new_entry = dict(s)\n",
    "    new_entry['content'] = new_content\n",
    "    cleaned.append(new_entry)\n",
    "    if any(changes.values()):\n",
    "        change_log.append({'idx': i, 'title': s.get('urdu_title',''), **changes})\n",
    "    for k,v in changes.items():\n",
    "        if isinstance(v, int) and v>0:\n",
    "            summary_counts[k] += v\n",
    "\n",
    "print('Clean complete — stories:', len(cleaned))\n",
    "print('Sentences removed:', summary_counts.get('removed_sentences', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "save_output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1780 stories to urdu_stories_final_preprocessed.json\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned JSON\n",
    "with open(CLEAN_JSON,'w',encoding='utf-8') as f:\n",
    "    json.dump(cleaned, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Saved', len(cleaned), 'stories to', CLEAN_JSON.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
