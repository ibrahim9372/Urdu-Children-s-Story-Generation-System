{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d8d386",
   "metadata": {},
   "source": [
    "# Notebook: 01 - Cleaning\n",
    "\n",
    "Purpose: conservative, reversible cleaning pipeline. Run the pipeline cell to regenerate `urdu_stories_final_preprocessed.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711dfc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & paths\n",
    "import json, re, unicodedata\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PHASE1_ROOT = NOTEBOOK_DIR\n",
    "SRC_JSON = PHASE1_ROOT / 'urdu_stories_final.json'\n",
    "if not SRC_JSON.exists():\n",
    "    raise FileNotFoundError(f\"{SRC_JSON} not found — run 01-data-collection.ipynb to create latest dataset\")\n",
    "CLEAN_DIR = PHASE1_ROOT\n",
    "CLEAN_JSON = CLEAN_DIR / 'urdu_stories_final_preprocessed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35bfdd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core normalization and cleaning utilities\n",
    "import re, unicodedata\n",
    "\n",
    "_DIACRITICS_RE = re.compile('[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]')\n",
    "\n",
    "def normalize_urdu(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    t = unicodedata.normalize('NFC', text)\n",
    "    t = _DIACRITICS_RE.sub('', t)\n",
    "    t = re.sub('[\\u0622\\u0623\\u0625\\u0671]', '\\u0627', t)  # Alef variants → ا\n",
    "    t = re.sub('\\u0643', '\\u06A9', t)  # ك -> ک\n",
    "    t = re.sub('\\u064A', '\\u06CC', t)  # ي -> ی\n",
    "    t = re.sub('[\\u200C\\u200D\\uFEFF]', '', t)\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def collapse_duplicate_markers(text: str) -> (str, int):\n",
    "    t = text\n",
    "    changed = 0\n",
    "    for tok in ['<EOS>','<EOP>','<EOT>']:\n",
    "        pattern = re.compile(r'(?:' + re.escape(tok) + r')[\\s\\n]*(?:' + re.escape(tok) + r')+')\n",
    "        t, n = pattern.subn(tok, t)\n",
    "        changed += n\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    return t.strip(), changed\n",
    "\n",
    "_TERMINATOR_RE = re.compile(r'([\\u06D4\\u061F\\.\\!\\?])(?!\\s*<EOS>)')\n",
    "\n",
    "def insert_missing_eos(text: str) -> (str, int):\n",
    "    inserts = 0\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for m in _TERMINATOR_RE.finditer(text):\n",
    "        pos = m.start(1)\n",
    "        prev_slice = text[last:pos+1]\n",
    "        if len(prev_slice.strip()) >= 8:\n",
    "            after = text[m.end(1): m.end(1)+10]\n",
    "            if '<EOS>' not in after:\n",
    "                inserts += 1\n",
    "                parts.append(text[last:m.end(1)] + ' <EOS>')\n",
    "                last = m.end(1)\n",
    "    parts.append(text[last:])\n",
    "    if inserts:\n",
    "        return ''.join(parts), inserts\n",
    "    return text, 0\n",
    "\n",
    "\n",
    "def clean_story_text(orig: str) -> (str, dict):\n",
    "    t = orig\n",
    "    changes = {'normalized': False, 'collapsed_markers': 0, 'inserted_eos': 0, 'added_eot': 0}\n",
    "    norm = normalize_urdu(t)\n",
    "    if norm != t:\n",
    "        changes['normalized'] = True\n",
    "        t = norm\n",
    "    t, collapsed = collapse_duplicate_markers(t)\n",
    "    changes['collapsed_markers'] = collapsed\n",
    "    t, inserted = insert_missing_eos(t)\n",
    "    changes['inserted_eos'] = inserted\n",
    "    if '<EOT>' not in t:\n",
    "        t = t.strip() + ' <EOT>'\n",
    "        changes['added_eot'] = 1\n",
    "    t = re.sub(r'\\s*<\\s*(EOP|EOS|EOT)\\s*>\\s*', r' <\\1> ', t)\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t, changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d94cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean complete — stories: 287 changed: 287 EOS inserted: 1543 (artifacts kept in-memory)\n"
     ]
    }
   ],
   "source": [
    "# Run cleaning pipeline\n",
    "with open(SRC_JSON,'r',encoding='utf-8') as f:\n",
    "    orig_stories = json.load(f)\n",
    "\n",
    "cleaned = []\n",
    "change_log = []\n",
    "summary_counts = Counter()\n",
    "for i,s in enumerate(orig_stories):\n",
    "    content = s.get('content','')\n",
    "    new_content, changes = clean_story_text(content)\n",
    "    new_entry = dict(s)\n",
    "    new_entry['content'] = new_content\n",
    "    cleaned.append(new_entry)\n",
    "    if any(changes.values()):\n",
    "        change_log.append({'idx': i, 'title': s.get('urdu_title',''), **changes})\n",
    "    for k,v in changes.items():\n",
    "        if isinstance(v, int) and v>0:\n",
    "            summary_counts[k] += v\n",
    "\n",
    "# write cleaned JSON (non-destructive)\n",
    "with open(CLEAN_JSON,'w',encoding='utf-8') as f:\n",
    "    json.dump(cleaned, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "report = {\n",
    "    'stories_total': len(cleaned),\n",
    "    'stories_changed': len(change_log),\n",
    "    'inserted_eos_total': summary_counts.get('inserted_eos',0),\n",
    "    'collapsed_markers_total': summary_counts.get('collapsed_markers',0),\n",
    "    'added_eot_total': summary_counts.get('added_eot',0)\n",
    "}\n",
    "\n",
    "print('Clean complete — stories:', len(cleaned), 'changed:', len(change_log), 'EOS inserted:', summary_counts.get('inserted_eos',0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
